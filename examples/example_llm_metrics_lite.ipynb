{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# llm-metrics-lite Example Notebook\n", "\n", "Demonstration of how to evaluate LLM outputs using the `llm-metrics-lite` Python package\n"]}, {"cell_type": "code", "metadata": {}, "source": ["from llm_metrics_lite import train_default_perplexity_model, evaluate_output\n", "\n", "corpus = [\n", "    \"Artificial intelligence enables machines to learn from data.\",\n", "    \"Language models process and generate human-like text.\",\n", "]\n", "\n", "ppl_model = train_default_perplexity_model(corpus)\n", "\n", "output_text = \"Generative AI models help automate reasoning tasks.\"\n", "context_text = \"AI models are capable of understanding language and generating responses.\"\n", "\n", "result = evaluate_output(output_text, context_text=context_text, perplexity_model=ppl_model)\n", "result"]}], "metadata": {"kernelspec": {"name": "python3", "language": "python", "display_name": "Python 3"}, "language_info": {"name": "python", "file_extension": ".py"}}, "nbformat": 4, "nbformat_minor": 5}