# llm-metrics-lite
![MIT License](https://img.shields.io/badge/License-MIT-yellow.svg)
![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)
![Status](https://img.shields.io/badge/Project-Active-brightgreen)

<h1 align="center">
  <br>
  <a href="https://github.com/supriyabachal/llm_metrics_lite">
    <img src="https://raw.githubusercontent.com/supriyabachal/llm_metrics_lite/main/docs/llm-metrics-lite-logo.png" alt="llm-metrics-lite" width="350">
  </a>
  <br>
</h1>

<h4 align="center">A lightweight, dependency-minimal evaluation toolkit for Large Language Model (LLM) output quality.</h4>

<p align="center">
  <a href="LICENSE">
    <img src="https://img.shields.io/badge/License-MIT-blue.svg" alt="License">
  </a>
  <a href="https://pypi.org/project/llm-metrics-lite/">
    <img src="https://img.shields.io/pypi/v/llm-metrics-lite?color=brightgreen" alt="Version">
  </a>
  <a href="https://pypi.org/project/llm-metrics-lite/">
    <img src="https://img.shields.io/pypi/pyversions/llm-metrics-lite.svg" alt="Python Versions">
  </a>
</p>

<br>

<p align="center">
  <a href="#quick-start">Quick Start</a> •
  <a href="#features">Features</a> •
  <a href="#examples">Examples</a> •
  <a href="#cli-usage">CLI Usage</a> •
  <a href="#roadmap">Roadmap</a> •
  <a href="#contributing">Contributing</a> •
  <a href="#license">License</a>
</p>

<p align="center">
  llm-metrics-lite provides simple, reliable metrics to evaluate generated text from LLMs.  
  It offers coherence scoring, reference-free perplexity, groundedness checks, token statistics, latency utilities, and a clean CLI interface — all without heavy dependencies.
</p>

---

## Quick Start

Install from PyPI:

```bash
pip install llm-metrics-lite
