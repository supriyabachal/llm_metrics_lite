# llm-metrics-lite
![MIT License](https://img.shields.io/badge/License-MIT-yellow.svg)
![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)
![Status](https://img.shields.io/badge/Project-Active-brightgreen)


llm-metrics-lite is a lightweight, dependency-minimal Python toolkit for evaluating text generated by Large Language Models (LLMs).
It provides coherence scoring, reference-free perplexity, groundedness heuristics, token statistics, and a simple command-line interface.

Why This Library Exists

Current LLM evaluation tools are often:

too complex or heavyweight

scattered across custom scripts

tied to specific frameworks

llm-metrics-lite offers a simple, general-purpose evaluation layer that:

works in research, production, and benchmarking

installs quickly

is easy to extend

requires no external language models

Features

Coherence Score
Measures similarity between consecutive sentences.

Reference-Free Perplexity
Character-level n-gram perplexity without model dependencies.

Groundedness / Factuality
Measures word overlap between output text and provided context.

Latency Utilities
Timer functions for benchmarking inference.

Token Statistics
Word count, character count, and approximate token estimation.

Command Line Interface
Evaluate text files directly from the terminal.

No Heavy Dependencies
Fast, portable, and framework agnostic.

Installation

Install from PyPI:

pip install llm-metrics-lite


Install from source:

pip install -e .

Basic Example
from llm_metrics_lite import train_default_perplexity_model, evaluate_output

corpus = [
    "Artificial intelligence enables machines to learn from data.",
    "Language models process and generate human-like text."
]

ppl_model = train_default_perplexity_model(corpus)

output = "Generative AI models help automate reasoning tasks."
context = "AI models are capable of understanding language and generating responses."

result = evaluate_output(
    output_text=output,
    context_text=context,
    perplexity_model=ppl_model
)

print(result)

Command Line Usage

Evaluate a text file:

llm-metrics-lite evaluate output.txt


Evaluate using context:

llm-metrics-lite evaluate output.txt --context context.txt

Project Roadmap

Upcoming features include:

Embedding-based coherence

Semantic groundedness

Hallucination scoring

Batch evaluation

API for comparing multiple model outputs

Benchmarking utilities
