{"nbformat": 4, "nbformat_minor": 5, "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"name": "python", "file_extension": ".py"}}, "cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# llm-metrics-lite Example Notebook\n", "\n", "This notebook demonstrates how to evaluate LLM output text using the `llm-metrics-lite` library.\n", "Outputs are **shown as comments**, not executed, so the notebook stays clean.\n"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["from llm_metrics_lite import train_default_perplexity_model, evaluate_output\n", "\n", "# Small corpus to train perplexity model\n", "corpus = [\n", "    \"Artificial intelligence enables machines to learn from data.\",\n", "    \"Language models process and generate human-like text.\",\n", "]\n", "\n", "ppl_model = train_default_perplexity_model(corpus)\n", "\n", "output_text = \"Generative AI models help automate reasoning tasks.\"\n", "context_text = \"AI models are capable of understanding language and generating responses.\"\n", "\n", "result = evaluate_output(output_text, context_text=context_text, perplexity_model=ppl_model)\n", "result  # Expected output shown below as comment\n", "\n", "# Example expected output:\n", "# LLMEvaluationResult(\n", "#    coherence=0.72,\n", "#    perplexity=15.3,\n", "#    groundedness=0.50,\n", "#    word_count=7,\n", "#    char_count=56,\n", "#    approx_token_count=9\n", "# )\n"]}]}